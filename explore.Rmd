---
title: "Explore"
author: "Mengyu Zhang / mz2777"
date: "3/2/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psycho)
library(pracma)
library(MASS)
library(glmnet)
```

```{r data}
breast_cancer <- data.frame(read_csv("project 2/breast-cancer.csv")[,-33])

cancer = breast_cancer %>% 
  mutate(diagnosis = recode(diagnosis, "M" = 1, "B" = 0))
cancer_ls = list(x = cancer[,3:32], y = cancer$diagnosis)
x = sapply(cancer_ls$x, function(x) as.numeric(unlist(x)))
cancer_ls = list(x = x, y = cancer$diagnosis)
```



```{r loglikelyhood}
dat = cancer_ls
betavec = rep(0, 31)
logisticstuff <- function(dat, betavec){
  x<- dat$x
  xm <- cbind(rep(1, nrow(x)), scale(x)) # standardize the data
  u <- xm %*% betavec
  expu <- exp(u)
  j <- 1
  loglik_temp <- matrix()
  p <- matrix()
  
  for(j in 1:569){
    loglik_temp[j] <- ifelse(u[j] > 10, sum(dat$y[j] * u[j]  - u[j]), sum(dat$y[j] * u[j]  - log(1 + expu[j])))
    p[j] <- ifelse(u[j] > 10, 1, expu[j] / (1 + expu[j]))
  j <- j+1
  }
 
  loglik <- sum(loglik_temp)
  grad <- matrix(colSums(xm * as.vector(dat$y - p))) # gradient at betavec
  Hess <- 0
  i = 1 
  for (i in 1:569) {
    tt <- xm[i,]
    dd <- t(tt)
    Hess <- Hess - tt %*% dd * p[i] * (1 - p[i])
    i <- i + 1
  }
  return(list(loglik = loglik, grad = grad, Hess = Hess))
}
```


```{r modified}
func = logisticstuff

NewtonRaphson <- function(dat, func, start, tol=1e-10, maxiter = 20000) {
  i <- 0
  cur <- start
  stuff <- func(dat, cur)
  res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf      # To make sure it iterates
  while(i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i <- i + 1
    prevloglik <- stuff$loglik
    prev <- cur
    cur <- prev - ginv(stuff$Hess, 2.34406e-18) %*% stuff$grad
    prevstuff <- stuff
    stuff <- func(dat, cur)        # log-lik, gradient, Hessian
    
    while(max(eigen(stuff$Hess)$value)>0){
      stuff$Hess = stuff$Hess - diag(31)*1
    }
    
    if (stuff$loglik > prevloglik)
    {
        res <- rbind(res, c(i, stuff$loglik, cur))# Add current values to results matrix
    } else 
    {
      lambda <- 1
      while (stuff$loglik < prevloglik) {
        lambda <- lambda / 2 # step-halving
        cur <- prev - lambda * ginv(prevstuff$Hess, 2.34406e-18) %*% prevstuff$grad
        stuff <- func(dat, cur)        # log-lik, gradient, Hessian
      }
        res <- rbind(res, c(i, stuff$loglik, cur))# Add current values to results matrix
    }
  }
  return(res)
}

```


```{r}
NewRes = NewtonRaphson(dat = cancer_ls, func = logisticstuff, start = rep(0, 31))

#res1 = NewtonRaphson(dat = cancer_ls, func = logisticstuff, start = rep(1, 31))

check <- tail(NewRes)[,1:2]

```





```{r coordinate-wise logistic lasso}

Sfunc <- function(beta,lambda) {

  if ((abs(beta)-lambda) > 0)
  {
    return (sign(beta) * (abs(beta)-lambda))
    }
  else {
    return (0)
    }
}


coordlasso <- function(lambda, dat, start, tol=1e-10, maxiter = 200){
  x<- dat$x
  xm <- cbind(rep(1, nrow(x)), scale(x)) # standardize the data
  i <- 0 
  pp <- length(start)
  n <- length(dat$y)
  betavec <- start
  loglik <- 0
  res <- c(0, loglik, betavec)
  prevloglik <- -Inf # To make sure it iterates 
  while (i < maxiter && abs(loglik - prevloglik) > tol && loglik < Inf) {
    i <- i + 1 
    prevloglik <- loglik
    for (j in 1:pp) {
      u <- xm %*% betavec
      expu <- exp(u) 
      prob <- expu / (expu + 1)
      w <- prob * (1 - prob) # weights
      # avoid coeffcients diverging in order to achieve fitted  probabilities of 0 or 1.
      w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
      z <- u + (dat$y - prob) / w
      # calculate noj
      z_j <- xm[,-j] %*% betavec[-j]
      betavec[j] <- Sfunc(sum(w * (xm[,j]) * (z - z_j)), lambda) / (sum(w * xm[,j] * xm[,j]))
    }
    loglik <- sum(w * (z - xm %*% betavec)^2) / (2 * n) + lambda * sum(abs(betavec))
    res <- rbind(res, c(i, loglik, betavec))}  
  return(res)
}
```


```{r}
CorRes <- coordlasso(lambda = exp(-8e-1), cancer_ls, start = rep(0, 31) ,maxiter = 2000)



logmod <- glmnet(cancer_ls$x, y=cancer_ls$y, alpha=1, family="binomial",lambda = 1e-2)
coef.glmnet(logmod)


```

```{r}

```



```{r}
path <- function(data,lambdas){
  start <- rep(0, 31)
  betas <- NULL
  for (x in 1:30) {
  coor.result <- coordlasso(lambda = lambdas[x],
                           dat = data,
                           start= start)
  curbeta <- coor.result[nrow(coor.result),3:dim(coor.result)[2]]
  start <- curbeta
  betas <- rbind(betas,c(curbeta))
  }
  return(data.frame(cbind(lambdas,betas)))
}

path.out <- path(cancer_ls,lambdas = exp(seq(-8e-1,-8, length=30)))
colnames(path.out) <- c("lambdas","intercept", colnames(cancer_ls$x))


# plot a path of solutions
path.plot <- path.out %>%
  pivot_longer(
    3:32,
    names_to = "meansures",
    values_to = "values"
  ) %>% 
  ggplot(aes(x = log(lambdas), y = values, group = meansures, color = meansures)) +
  geom_line() + 
  theme_bw() +
  ggtitle("A path of solutions with a sequence of descending lambda's") +
  xlab("log(Lambda)") + 
  ylab("Values") +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 6))
path.plot + scale_x_reverse()


```

```{r}
x<- dat$x
newX <- cbind(rep(1, nrow(x)), scale(x)) # standardize the data
resp <- cancer_ls$y
##### 5-fold cross-validation and pathwise coordinatewise optimization algorithm
# when lambda = 0.5, all the betas go to 0
cvresult <- function(inputx,inputy,grid,K){
  n <- dim(inputx)[1]
  folds <- sample(1:K, n, replace=TRUE)
  start <- rep(0, dim(inputx)[2])
  cv.error <- vector()
  cv.se <- vector()
  for (x in 1:length(grid)) {
  cv.errors <- vector()
  for (i in 1:K){
    cor.result <- coordlasso(lambda = grid[x], 
                                  dat = list(X=as.matrix(inputx[folds!=i,]),y=inputy[folds!=i]),
                                  s = start)
    lasbeta <- cor.result[nrow(cor.result),3:dim(cor.result)[2]]
    u <- as.matrix(inputx[folds == i,]) %*% lasbeta
    expu <- exp(u) 
    prob <- expu / (1 + expu) 
    y <- as.vector(inputy[folds==i])
    cv.errors[i] = mean((y-prob)^2) #MSE
    start <- lasbeta
  }
  cv.error[x] <- mean(cv.errors)
  cv.se[x] <- sqrt(var(cv.errors)/K)
  }
  return(cbind(grid,cv.error,cv.se))
}

result <- cvresult(newX,resp,grid=exp(seq(-8e-1,-8, length=100)),K=5)
# result <- cvresult(X,resp,grid=seq(0.5, 1e-2, length=100),K=5)
best.lambda <- result[which.min(result[,2]),1]
# need rewrite
finlasso <- as.matrix(path(cancer_ls,lambdas = exp(seq(-8e-1,log(best.lambda), length = 100))))
lasso.beta <- finlasso[nrow(finlasso),2:dim(finlasso)[2]]

# plot for cross validation
result <- data.frame(result)
cv.plot <- 
    ggplot(result, aes(x=log(result$grid), y=result$cv.error)) + 
    geom_errorbar(aes(ymin=result$cv.error-result$cv.se, ymax=result$cv.error+result$cv.se),
                  colour=1) +
    geom_line() +
    geom_point(size=0.8,colour = 4) +
    ggtitle("Figure 3: Lasso regression by 5 fold cross validation")+
    xlab("log(Lambda)") + ylab("MSE") +
    geom_vline(xintercept = log(best.lambda),col=3,lty=3) +
    annotate("text", log(best.lambda), 0.1, label = paste("best log(lambda) = ", round(log(best.lambda), 3), sep = ""))
cv.plot


set.seed(99999)
logmod <- cv.glmnet(cancer_ls$x, y=cancer_ls$y, alpha=1, family="binomial",type.measure="mse")
# check: plot(logmod)
# check: logmod$lambda.min
```

```{r}
pred.fun <- function(outcome,input, beta){
    u <- as.matrix(input) %*% beta
    expu <- exp(u) 
    prob <- expu / (1 + expu) 
    pred.error = mean((as.vector(outcome)-prob)^2)
    return(pred.error)
}


lasso.ite <- nrow(finlasso)
lasso.beta <- lasso.beta
lasso.pred <-  pred.fun(cancer_ls$y,newX,lasso.beta)
```

